
# ğŸ¤– TechStack Advisor â€“ RAG Challenge mit Next.js & Supabase

TechStack Advisor ist ein modernes Retrieval-Augmented-Generation (RAG) System, das Entwicklerfragen auf Basis einer eigenen Wissensdatenbank beantwortet. Es kombiniert Next.js (Frontend & API) mit Supabase (Postgres + pgvector) fÃ¼r semantische Suche und persistente Speicherung. Die Antworten des LLMs basieren ausschlieÃŸlich auf den gefundenen Dokumenten â€“ mit transparenter Quellenangabe und nachvollziehbarem Logging.

## Architektur
1. User stellt Frage im Web-UI (Next.js/React)
2. API-Route erzeugt ein Embedding und sucht in Supabase nach Ã¤hnlichen Dokumenten
3. Die gefundenen Dokumente werden als Kontext an das LLM gegeben
4. Das LLM generiert eine Antwort ausschlieÃŸlich aus diesen Quellen
5. Antwort & Quellen werden im UI angezeigt

## Setup & Start
1. **Supabase-Projekt anlegen** und pgvector-Erweiterung aktivieren
2. **Migrationen ausfÃ¼hren:**
  - `supabase/migrations/20260124_init_schema.sql`
3. **Wissensdatenbank befÃ¼llen:**
  - `npm run seed:tech` (fÃ¼hrt `scripts/seed-knowledge.ts` aus)
4. **Umgebungsvariablen setzen:** `.env.local` mit Supabase- und OpenAI-Keys
5. **App starten:**
  - `npm install`
  - `npm run dev`

## Design-Entscheidungen
- **Trennung von Infrastruktur (lib/) und Business-Logik (services/)**
- **Klares Prompt-Design:** LLM darf nur aus echten Dokumenten antworten
- **Erweiterbar:** Multi-Tenant, weitere Dokumente, andere LLMs mÃ¶glich

## Demo
- Fragebeispiele: "Was ist ein React Server Component?", "Wie funktioniert die Vektorsuche?"
- Quellenangaben werden im Chat angezeigt


---

## ğŸ§© Chunking-Logik: splitTextIntoChunks

Um lange Texte effizient fÃ¼r die semantische Suche vorzubereiten, nutzt das System eine eigene Chunking-Funktion:

- **splitTextIntoChunks** teilt groÃŸe Texte in Ã¼berlappende Abschnitte (Chunks), z.B. 512 WÃ¶rter pro Chunk mit 50 WÃ¶rtern Ãœberlappung (Standardwerte).
- Die Chunk-GrÃ¶ÃŸe und Ãœberlappung sind konfigurierbar (siehe `APP_CONFIG` in `lib/constants.ts`).
- Die Funktion approximiert Token durch WÃ¶rter â€“ das ist fÃ¼r OpenAI-Embeddings ausreichend genau.
- Vorteil: Auch lÃ¤ngere Dokumente werden vollstÃ¤ndig und mit Kontextabdeckung indiziert, ohne dass relevante Informationen an Chunk-Grenzen verloren gehen.
- Die Chunks werden als einzelne EintrÃ¤ge in der Vektordatenbank gespeichert und bei der Suche als Kontext fÃ¼r das LLM verwendet.

**Beispiel:**

```ts
import { splitTextIntoChunks } from './lib/utils';

const text = '...langer Text...';
const chunks = splitTextIntoChunks(text, 512, 50);
// â†’ Gibt ein Array von Strings zurÃ¼ck, jeder String ist ein Chunk
```

---

**Challenge umgesetzt von:**
[Stefan Helldobler](https://stefan-helldobler.de/portfolio/) | [https://github.com/Stefan374245/RAG-with-Next.js-Supabase]

[![Next.js](https://img.shields.io/badge/Next.js-15-black)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-19-blue)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)
[![Supabase](https://img.shields.io/badge/Supabase-PostgreSQL-green)](https://supabase.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-purple)](https://openai.com/)

---

## ğŸ“– Was ist TechStack Advisor?

**TechStack Advisor** ist ein KI-gestÃ¼tzter Assistent, der speziell fÃ¼r Entwickler entwickelt wurde. Statt allgemeines Wissen zu nutzen, durchsucht das System eine **eigene Wissensdatenbank** mit technischer Dokumentation und liefert **prÃ¤zise, quellenbasierte Antworten**.

### ğŸ¯ Kernfunktionen

âœ… **RAG-basierte Antworten** - Nutzt NUR Informationen aus der Wissensdatenbank, kein Halluzinieren  
âœ… **Semantische Suche** - Findet relevante Dokumente durch VektorÃ¤hnlichkeit (pgvector)  
âœ… **Live-Streaming** - Antworten werden in Echtzeit gestreamt  
âœ… **Quellenangaben** - Zeigt verwendete Dokumente mit Relevanz-Scores  
âœ… **Chat-Historie** - Alle GesprÃ¤che werden gespeichert und kÃ¶nnen wiederhergestellt werden  
âœ… **Modern Stack** - Next.js 15, React 19, TypeScript, Supabase, OpenAI  

---

## ğŸš€ Wie funktioniert es?

### RAG-Pipeline in 3 Schritten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RETRIEVAL (Abrufen)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  User-Frage â†’ Embedding generieren â†’ Vector Search              â”‚
â”‚  "ErklÃ¤re mir Angular" â†’ [0.032, -0.009, ...] â†’ Top 5 Docs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AUGMENTATION (Anreichern)                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Gefundene Dokumente â†’ System-Prompt injizieren                 â”‚
â”‚  [Angular Components, Services...] â†’ Kontext fÃ¼r LLM           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GENERATION (Generieren)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Angereicherter Prompt â†’ GPT-4 â†’ Streaming Response             â”‚
â”‚  System + User Query â†’ LLM â†’ "Angular ist..." + Quellen [1][2] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” Detaillierter Workflow

1. **User stellt Frage:** "ErklÃ¤re mir Angular Components"
   
2. **Embedding-Generierung:**
   - OpenAI `text-embedding-3-small` erstellt 1536-dimensionalen Vektor
   - Query wird in mathematische ReprÃ¤sentation umgewandelt

3. **Vektor-Suche (Supabase):**
   - PostgreSQL mit pgvector Extension
   - HNSW-Index fÃ¼r schnelle Cosine-Similarity-Suche
   - Top 5 Ã¤hnlichste Dokumente werden abgerufen (Threshold: 30%)

4. **Kontext-Injektion:**
   - Gefundene Dokumente werden in System-Prompt eingefÃ¼gt
   - LLM bekommt strikte Anweisung: **"Nutze NUR diese Dokumente!"**

5. **LLM-Generierung:**
   - GPT-4 Turbo generiert Antwort basierend auf Kontext
   - Temperature: 0.3 (sehr faktisch, wenig kreativ)
   - Streaming via Server-Sent Events (SSE)

6. **Response mit Quellen:**
   - User sieht Antwort + Quellenangaben [1], [2]
   - Kann Dokumente mit Relevanz-Scores einsehen
