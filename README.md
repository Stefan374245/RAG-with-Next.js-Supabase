# ğŸ¤– TechStack Advisor - Intelligent Developer Documentation Assistant

> Ein produktionsreifes **Retrieval-Augmented Generation (RAG)** System, das Entwicklern prÃ¤zise, quellenbasierte Antworten auf technische Fragen liefert.

[![Next.js](https://img.shields.io/badge/Next.js-15-black)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-19-blue)](https://react.dev/)
[![TypeScript](https://img.shields.io/badge/TypeScript-5-blue)](https://www.typescriptlang.org/)
[![Supabase](https://img.shields.io/badge/Supabase-PostgreSQL-green)](https://supabase.com/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4-purple)](https://openai.com/)

---

## ğŸ“– Was ist TechStack Advisor?

**TechStack Advisor** ist ein KI-gestÃ¼tzter Assistent, der speziell fÃ¼r Entwickler entwickelt wurde. Statt allgemeines Wissen zu nutzen, durchsucht das System eine **eigene Wissensdatenbank** mit technischer Dokumentation und liefert **prÃ¤zise, quellenbasierte Antworten**.

### ğŸ¯ Kernfunktionen

âœ… **RAG-basierte Antworten** - Nutzt NUR Informationen aus der Wissensdatenbank, kein Halluzinieren  
âœ… **Semantische Suche** - Findet relevante Dokumente durch VektorÃ¤hnlichkeit (pgvector)  
âœ… **Live-Streaming** - Antworten werden in Echtzeit gestreamt  
âœ… **Quellenangaben** - Zeigt verwendete Dokumente mit Relevanz-Scores  
âœ… **Chat-Historie** - Alle GesprÃ¤che werden gespeichert und kÃ¶nnen wiederhergestellt werden  
âœ… **Modern Stack** - Next.js 15, React 19, TypeScript, Supabase, OpenAI  

---

## ğŸš€ Wie funktioniert es?

### RAG-Pipeline in 3 Schritten

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. RETRIEVAL (Abrufen)                                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  User-Frage â†’ Embedding generieren â†’ Vector Search              â”‚
â”‚  "ErklÃ¤re mir Angular" â†’ [0.032, -0.009, ...] â†’ Top 5 Docs     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. AUGMENTATION (Anreichern)                                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Gefundene Dokumente â†’ System-Prompt injizieren                 â”‚
â”‚  [Angular Components, Services...] â†’ Kontext fÃ¼r LLM           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. GENERATION (Generieren)                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
â”‚  Angereicherter Prompt â†’ GPT-4 â†’ Streaming Response             â”‚
â”‚  System + User Query â†’ LLM â†’ "Angular ist..." + Quellen [1][2] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ” Detaillierter Workflow

1. **User stellt Frage:** "ErklÃ¤re mir Angular Components"
   
2. **Embedding-Generierung:**
   - OpenAI `text-embedding-3-small` erstellt 1536-dimensionalen Vektor
   - Query wird in mathematische ReprÃ¤sentation umgewandelt

3. **Vektor-Suche (Supabase):**
   - PostgreSQL mit pgvector Extension
   - HNSW-Index fÃ¼r schnelle Cosine-Similarity-Suche
   - Top 5 Ã¤hnlichste Dokumente werden abgerufen (Threshold: 30%)

4. **Kontext-Injektion:**
   - Gefundene Dokumente werden in System-Prompt eingefÃ¼gt
   - LLM bekommt strikte Anweisung: **"Nutze NUR diese Dokumente!"**

5. **LLM-Generierung:**
   - GPT-4 Turbo generiert Antwort basierend auf Kontext
   - Temperature: 0.3 (sehr faktisch, wenig kreativ)
   - Streaming via Server-Sent Events (SSE)

6. **Response mit Quellen:**
   - User sieht Antwort + Quellenangaben [1], [2]
   - Kann Dokumente mit Relevanz-Scores einsehen

---

## ğŸ—ï¸ System-Architektur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FRONTEND (React 19)                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ ChatWindow   â”‚  â”‚ SourceList   â”‚  â”‚ History      â”‚        â”‚
â”‚  â”‚ (useChat)    â”‚  â”‚ (Citations)  â”‚  â”‚ (Sessions)   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ HTTP / SSE
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  API LAYER (Edge Runtime)                      â”‚
â”‚  /api/chat          - RAG Orchestration + Streaming            â”‚
â”‚  /api/db-health     - Database Health Check                    â”‚
â”‚  /api/test-rag      - Integration Testing                      â”‚
â”‚  /api/debug-prompt  - Prompt Inspection                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   SUPABASE POSTGRES      â”‚   â”‚   OPENAI API         â”‚
â”‚   â”œâ”€â”€ knowledge_base     â”‚   â”‚   â”œâ”€â”€ Embeddings     â”‚
â”‚   â”‚   â”œâ”€â”€ title         â”‚   â”‚   â”‚   (1536-dim)      â”‚
â”‚   â”‚   â”œâ”€â”€ content       â”‚   â”‚   â””â”€â”€ GPT-4 Turbo    â”‚
â”‚   â”‚   â”œâ”€â”€ embedding     â”‚   â”‚       (streaming)     â”‚
â”‚   â”‚   â””â”€â”€ metadata      â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚   â”œâ”€â”€ chat_history      â”‚
â”‚   â””â”€â”€ pgvector + HNSW   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“¦ Service Layer (Clean Architecture)

```typescript
features/rag-chat/services/
â”œâ”€â”€ vector-service.ts       // Embedding + Vector Search
â”‚   â”œâ”€â”€ generateEmbedding()
â”‚   â”œâ”€â”€ searchKnowledge()
â”‚   â””â”€â”€ storeDocument()
â”‚
â”œâ”€â”€ llm-service.ts          // Prompt Engineering
â”‚   â””â”€â”€ buildRAGPrompt()    // Strenges RAG-Prompt Template
â”‚
â””â”€â”€ chat-history-service.ts // Session Management
    â”œâ”€â”€ saveMessage()
    â”œâ”€â”€ loadSession()
    â””â”€â”€ getAllSessions()
```

---

## ğŸ› ï¸ Tech Stack

### Core Framework
- **Next.js 15** - App Router, React Server Components, Edge Runtime
- **React 19** - React Compiler fÃ¼r Auto-Memoization
- **TypeScript 5** - Strict Mode, vollstÃ¤ndige Type Safety

### AI & Vector Search
- **OpenAI API** - `text-embedding-3-small` (Embeddings), `gpt-4-turbo` (Chat)
- **Vercel AI SDK 3.0** - `streamText()`, `useChat()` Hook
- **Supabase** - PostgreSQL + pgvector Extension fÃ¼r Vector Search
- **HNSW Index** - Hierarchical Navigable Small World fÃ¼r schnelle Similarity Search

### Styling & UI
- **Tailwind CSS** - Utility-First Styling
- **Lucide React** - Icon Library
- **Custom Animations** - Fade-in, Slide-in, Glow Effects

### Development
- **ESLint** - Code Linting
- **TypeScript Strict** - Keine `any` Types
- **Centralized Logger** - Strukturiertes Logging mit Context

---

## ğŸ“ Projekt-Struktur (Feature-Based Architecture)

```
/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat/route.ts              # ğŸ”¥ Main RAG Endpoint
â”‚   â”‚   â”œâ”€â”€ db-health/route.ts         # Database Health Check
â”‚   â”‚   â”œâ”€â”€ test-rag/route.ts          # RAG Testing Endpoint
â”‚   â”‚   â””â”€â”€ debug-prompt/route.ts      # Prompt Debugging
â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â”œâ”€â”€ ingest.action.ts           # Document Ingestion
â”‚   â”‚   â””â”€â”€ seed.action.ts             # Knowledge Base Seeding
â”‚   â”œâ”€â”€ admin/page.tsx                 # Admin Panel
â”‚   â”œâ”€â”€ layout.tsx                     # Root Layout
â”‚   â””â”€â”€ page.tsx                       # Landing Page
â”‚
â”œâ”€â”€ features/rag-chat/                 # ğŸ¯ Main RAG Feature
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ chat-window.tsx            # Main Chat Interface
â”‚   â”‚   â”œâ”€â”€ chat-history-dropdown.tsx  # History Dropdown
â”‚   â”‚   â”œâ”€â”€ chat-history-sidebar.tsx   # Full History View
â”‚   â”‚   â”œâ”€â”€ message-bubble.tsx         # Message Display
â”‚   â”‚   â”œâ”€â”€ source-list.tsx            # Citation Display
â”‚   â”‚   â”œâ”€â”€ chat-input.tsx             # User Input
â”‚   â”‚   â””â”€â”€ system-info-overlay.tsx    # System Info Modal
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ vector-service.ts          # ğŸ”¥ Vector Operations
â”‚   â”‚   â”œâ”€â”€ llm-service.ts             # ğŸ”¥ Prompt Engineering
â”‚   â”‚   â””â”€â”€ chat-history-service.ts    # History Management
â”‚   â””â”€â”€ types.ts                       # Feature Types
â”‚
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ supabase.ts                    # Supabase Client
â”‚   â”œâ”€â”€ openai.ts                      # OpenAI Configuration
â”‚   â”œâ”€â”€ constants.ts                   # App Configuration
â”‚   â”œâ”€â”€ logger.ts                      # ğŸ†• Centralized Logging
â”‚   â””â”€â”€ utils.ts                       # Utilities
â”‚
â”œâ”€â”€ types/
â”‚   â””â”€â”€ index.ts                       # Global Type Definitions
â”‚
â”œâ”€â”€ supabase/
â”‚   â””â”€â”€ migrations/
â”‚       â””â”€â”€ 20260124_init_schema.sql   # Database Schema
â”‚
â””â”€â”€ scripts/
    â””â”€â”€ seed-knowledge.ts              # Knowledge Base Seeding
```

---

## âš¡ Quick Start

### 1. Installation

```bash
git clone <repo-url>
cd "RAG AI"
npm install
```

### 2. Environment Variables

Erstelle `.env.local`:

```env
# Supabase
NEXT_PUBLIC_SUPABASE_URL=https://xxxxx.supabase.co
NEXT_PUBLIC_SUPABASE_ANON_KEY=eyJhbGci...
SUPABASE_SERVICE_ROLE_KEY=eyJhbGci...

# OpenAI
OPENAI_API_KEY=sk-proj-...
```

### 3. Datenbank Setup

1. Erstelle Supabase Projekt auf [supabase.com](https://supabase.com)
2. FÃ¼hre Migration aus: `supabase/migrations/20260124_init_schema.sql`
3. Optional: Seed-Daten laden (siehe Admin Panel)

### 4. Starten

```bash
npm run dev
```

Ã–ffne [http://localhost:3000](http://localhost:3000)

---

## ğŸ¨ Features im Detail

### âœ… RAG-System

**Warum RAG statt direktes LLM?**
- âŒ **Standard LLM:** "Angular wurde 2010 von Google verÃ¶ffentlicht..." (Halluzination mÃ¶glich)
- âœ… **RAG System:** "Laut Dokument [1]: Angular Components sind..." (quellenbasiert)

**Vorteile:**
- PrÃ¤zise Antworten aus eigener Dokumentation
- Keine Halluzinationen bei fehlendem Wissen
- AktualitÃ¤t durch eigene Datenbank
- Nachvollziehbarkeit durch Quellenangaben

### ğŸ” Semantic Search

**Wie funktioniert Vektorsuche?**

```python
# Traditionelle Keyword-Suche
Query: "React Komponenten"
Findet: Nur Dokumente mit exakten WÃ¶rtern "React" + "Komponenten"

# Vektor-Suche (Semantic)
Query: "React Komponenten"
Findet auch: "React Components", "Function Components", 
             "Class Components", "JSX Elements"
             
â†’ Versteht Bedeutung, nicht nur Keywords!
```

**Performance:**
- HNSW-Index: ~100ms fÃ¼r 10.000 Dokumente
- Cosine Similarity: Mathematische Ã„hnlichkeitsberechnung
- Top-K Retrieval: Nur relevanteste Dokumente

### ğŸ’¬ Chat-Historie

**Features:**
- Alle GesprÃ¤che in Datenbank gespeichert
- Session-basierte Organisation (UUID)
- Dropdown fÃ¼r schnellen Zugriff
- Sidebar fÃ¼r vollstÃ¤ndige Historie
- LÃ¶schen einzelner Sessions mÃ¶glich

**Technische Details:**
```typescript
// Session-Struktur
interface ChatSession {
  session_id: string          // UUID
  last_message: string        // Preview
  created_at: string          // Timestamp
  message_count: number       // Anzahl Nachrichten
}

// Message-Struktur
interface ChatMessage {
  id: string
  session_id: string
  message: string
  role: 'user' | 'assistant'
  sources?: KnowledgeItem[]
  created_at: string
}
```

### ğŸ“Š Debug-Endpunkte

```bash
# 1. RAG-Pipeline testen
GET /api/test-rag?q=Angular
â†’ Zeigt Embeddings, RPC-Call, gefundene Dokumente

# 2. Prompt inspizieren
GET /api/debug-prompt?q=React
â†’ Zeigt exakten System-Prompt den das LLM erhÃ¤lt

# 3. Datenbank-Status
GET /api/db-health
â†’ PrÃ¼ft Tabellen, Embeddings, RPC-Funktionen
```

---

## ğŸš€ Deployment

### Vercel (Empfohlen)

**1. Repository verbinden**
```bash
git push origin main
```

**2. In Vercel:**
- Import Repository
- Framework: Next.js (auto-detected)
- Root Directory: `.`

**3. Environment Variables setzen:**

Alle Variablen aus `.env.local` in Vercel Project Settings â†’ Environment Variables Ã¼bertragen.

**4. Deploy:**
- Click "Deploy"
- Vercel erstellt automatisch URL: `https://xxx.vercel.app`

### âœ… Production Checklist

- [x] Environment Variables gesetzt
- [x] Supabase Projekt produktionsbereit
- [x] OpenAI API-Key mit ausreichend Credits
- [x] Knowledge Base mit Dokumenten gefÃ¼llt
- [x] Build lokal getestet (`npm run build`)
- [x] CORS/Headers konfiguriert (falls nÃ¶tig)

### ğŸ“Š Monitoring

Nach Deployment prÃ¼fen:
- Vercel Analytics aktiviert
- Error Tracking (Sentry optional)
- Database Metrics in Supabase
- OpenAI Usage Dashboard

**Logs ansehen:**
```
Vercel Dashboard â†’ Deployments â†’ [Latest] â†’ Runtime Logs
```

---

## ğŸ”§ Configuration

### RAG-Parameter anpassen

`lib/constants.ts`:

```typescript
export const APP_CONFIG = {
  // Ã„hnlichkeits-Schwellenwert (hÃ¶her = strenger)
  MATCH_THRESHOLD: 0.3,    // 30% Minimum
  
  // Anzahl Dokumente pro Query
  MATCH_COUNT: 5,          // Top 5
  
  // LLM Temperature (0 = faktisch, 1 = kreativ)
  TEMPERATURE: 0.3,        // Sehr faktisch
  
  // Max Response LÃ¤nge
  MAX_TOKENS: 1000,
  
  // Document Chunking
  CHUNK_SIZE: 512,         // Token pro Chunk
  CHUNK_OVERLAP: 50,       // Overlap fÃ¼r Kontext
}
```

### Prompt-Engineering

`features/rag-chat/services/llm-service.ts`:

```typescript
// VerschÃ¤rfung des Prompts
return `Du bist TechStack Advisor.

â›” ABSOLUTE REGELN:
1. Nutze AUSSCHLIESSLICH die folgenden Dokumente
2. IGNORIERE dein vortrainiertes Wissen
3. Bei fehlenden Infos: Sage das klar

ğŸ“š DOKUMENTE:
${context}

â“ FRAGE: ${userQuery}
ğŸ’¬ ANTWORT (NUR aus den Dokumenten!):`
```

---

## ğŸ§ª Testing

### Unit Tests (Empfohlen)

```bash
npm install -D vitest @testing-library/react
```

```typescript
// vector-service.test.ts
describe('generateEmbedding', () => {
  it('should return 1536-dimensional vector', async () => {
    const embedding = await generateEmbedding('test')
    expect(embedding).toHaveLength(1536)
  })
})
```

### Integration Tests

```bash
# RAG-Pipeline Ende-zu-Ende
curl http://localhost:3000/api/test-rag?q=React

# Database Health
curl http://localhost:3000/api/db-health

# Prompt Inspection
curl http://localhost:3000/api/debug-prompt?q=Angular
```

### E2E Tests (Playwright)

```bash
npm install -D @playwright/test
```

```typescript
test('RAG workflow', async ({ page }) => {
  await page.goto('http://localhost:3000')
  await page.fill('textarea', 'ErklÃ¤re mir React')
  await page.click('button[type="submit"]')
  await expect(page.locator('.message-bubble')).toContainText('React')
  await expect(page.locator('.source-list')).toBeVisible()
})
```

---

## ğŸ› Troubleshooting

### HÃ¤ufige Probleme

**1. "No sources found"**
```bash
# PrÃ¼fen ob Dokumente vorhanden
curl http://localhost:3000/api/db-health

# Threshold zu hoch?
# â†’ In constants.ts MATCH_THRESHOLD von 0.3 auf 0.2 senken
```

**2. "Supabase RPC error"**
```sql
-- In Supabase SQL Editor prÃ¼fen:
SELECT * FROM knowledge_base LIMIT 1;

-- Extension installiert?
CREATE EXTENSION IF NOT EXISTS vector;
```

**3. "Streaming doesn't work"**
```bash
# Edge Runtime Problem?
# â†’ npm run build && npm start (Production Mode testen)

# Network Tab prÃ¼fen: SSE-Verbindung sichtbar?
```

**4. "Chat History not saving"**
```typescript
// Browser Console:
localStorage.getItem('chat-sessions')
// â†’ Sollte JSON mit Sessions zeigen

// Supabase prÃ¼fen:
SELECT * FROM chat_history ORDER BY created_at DESC LIMIT 10;
```

---

## ğŸ“š WeiterfÃ¼hrende Ressourcen

### Dokumentation
- [RAG Best Practices](https://www.anthropic.com/index/retrieval-augmented-generation)
- [pgvector Guide](https://github.com/pgvector/pgvector)
- [Vercel AI SDK Docs](https://sdk.vercel.ai/docs)
- [Next.js 15 Documentation](https://nextjs.org/docs)

### Tutorials
- [Building Production RAG](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Vector Search Explained](https://www.youtube.com/watch?v=klTvEwg3oJ4)
- [React 19 Features](https://react.dev/blog/2024/12/05/react-19)

---

## ğŸ¯ Best Practices

### Clean Code
âœ… Feature-based Architecture  
âœ… Service Layer Pattern  
âœ… Centralized Logging  
âœ… Type Safety (no `any`)  
âœ… Error Boundaries  

### Performance
âœ… React Compiler (Auto-Memoization)  
âœ… Server Components  
âœ… Edge Runtime  
âœ… Vector Indexing (HNSW)  
âœ… Streaming Responses  

### Security
âœ… Environment Variables  
âœ… Input Validation  
âœ… Error Message Sanitization  
âœ… Rate Limiting (TODO)  
âœ… Authentication (TODO)  

---

## ğŸš€ Roadmap

### v2.0 (Planned)
- [ ] Multi-User Support (Supabase Auth)
- [ ] Advanced Analytics Dashboard
- [ ] Document Upload via UI
- [ ] Custom Embedding Models
- [ ] Hybrid Search (Vector + Keyword)
- [ ] Chat Export (PDF/Markdown)
- [ ] API Authentication
- [ ] Rate Limiting
- [ ] Caching Layer (Redis)

### v3.0 (Future)
- [ ] Multi-Language Support
- [ ] Voice Input/Output
- [ ] Collaborative Chats
- [ ] Plugin System
- [ ] Mobile App

---

## ğŸ“„ License

MIT License - siehe [LICENSE](LICENSE)

---

## ğŸ™ Acknowledgments

Dieses Projekt wurde entwickelt mit:
- **Vercel** - AI SDK & Hosting
- **Supabase** - PostgreSQL + pgvector
- **OpenAI** - GPT-4 & Embeddings
- **React Team** - React 19 & Compiler

---

## ğŸ“§ Support

Bei Fragen oder Problemen:
- ğŸ“– Lies die [Dokumentation](#-wie-funktioniert-es)
- ğŸ› Check [Troubleshooting](#-troubleshooting)
- ğŸ’¬ Ã–ffne ein Issue auf GitHub

---

**Built with â¤ï¸ for Developers**

â­ **Star this repo** if you find it helpful!
